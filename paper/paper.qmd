---
title: "An AI is Haunting Campus^[Karl Marx wrote about a \"spectre...haunting Europe--the spectre of communism\"; GenAI looks awfully like the spectre of unregulated surveillance capitalism.]: What Role for the Human in Higher Education?"
format: docx
bibliography:
  - "`r system('kpsewhich ../bibliography/bibliography.bib', intern=TRUE)`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)
```

```{r include=FALSE}
library(cowplot) # Streamlined Plot Theme and Plot Annotations for 'ggplot2'
library(dplyr) # A Grammar of Data Manipulation
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(knitr)
library(magick) # Advanced Graphics and Image-Processing in R
library(neuralnet) # Training of Neural Networks
```

## Abstract

The use of Generative Artificial Intelligence (AI) in higher education has become a sensational topic. AI has been presented as an everything tool that can do a fabulous array of tasks, from creating content to grading to providing feedback to students. This presentation of AI is misleading, and there is a need to examine the claims about what the technology can do and what it actually does in the context of teaching and learning. In this essay I scan the academic and gray literature, along with media reports regarding AI, to show how discussions about AI hinge on making a large number of unverified claims (Gish Gallop) while presenting the technology as inevitable (a False Dilemma). However, once we examine what AI does, and particularly what it does for whom and to whom, a picture emerges of a technology that is intrinsically inimical to the principles that most institutions of higher education claim to hold dear, including the best interest of their students.

## Introduction

On November 30, 2022, OpenAI released what became its flagship product, [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT), a chatbot based on [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) designed and trained to mimic natural language. The launch of the chatbot was an impressive feat of marketing [@cowen_chatgpt_2023], and it took less than two weeks for the hype to reach stratospheric levels [@roose_brilliance_2023; @lock_what_2022]. Soon after, everyone wanted in on the Generative AI (GenAI) craze^[The pattern of cyclical hypes around AI even has a name: [AI winters](https://en.wikipedia.org/wiki/AI_winter); the current one, with chatbots leading the parade, is just the most recent cycle of hype [see @siegel_ai_2023].]. 

AI technologies in education "offer new tools and applications that have the potential to transform traditional teaching and learning methods" [@adiguzel_revolutionizing_2023, p. 1], and have "the potential to fundamentally change the classroom experience and the knowledge and skills outcomes of our students" [@dwivedi_opinion_2023, p.20]. Furthermore, the technology "is here to stay...[t]herefore, teachers and students must develop the specific digital competencies needed to use such tools in ways that are pedagogically beneficial and ethical" [@kohnke_chatgpt_2023, p. 546]. 

AI is painted as revolutionary: an _innovative_ and _exciting_ technology, capable of _enhancing_ research and education, while leading to _efficiencies_^["Efficiencies" being an euphemism for cutting costs [@oconnell_let_2012, p. 64], usually by finding "redundancies" rather than better things for people to do.]. Higher education institutions were compelled to get in on the act, forming high-level advisory committees and drafting guidelines for university communities to leverage AI [e.g., @office_of_teaching_and_learning_provisional_2023; @mcmaster_university_provisional_2024; @trent_university_generative_2024; @university_of_toronto_ai_2024].

But hammers are powerful tools, and so are thousands of other instruments found at universities. And yet not all of them get high-level university committees to advice on their use. Just what is it about AIs that requires pan-university consultations on how to use them? To understand why practically every institution of higher education has decided to spend considerable resources studying the adoption and responsible use of AI, one needs to understand what the technology claims to do that sets it apart from other tools--and then, also examine what it actually does. 

In doing this, we must remain alert to the fact that technologies are never politically neutral, and two questions must be asked:

- What does a specific technology do for whom?

- And what does it do to whom?

As well, we need to understand the character of the organization that considers the use of the technology.

This essay examines claims about the uses of GenAIs in education, a sober assessment of what they actually do, while keeping in mind the mission of nominally principled organizations. I conclude that promises about GenAI are overstated and unlikely to be met, and that current AIs are antithetical to the values that universities claim to hold dear.

# The mission {#the-mission}

Universities pride themselves of serving "the social, cultural, and economic needs of [their] community and [their] society" [@office_of_the_president_mcmaster_university_mission_2024]; of striving "to make valued and socially responsible contributions to our local communities, to Canada, and to the world" [@trent_university_vision_2024]; and of "[serving] society and...[enhancing] the quality of life through scholarship" [@university_of_guelph_mission_2024]. 

These goals are achieved through "the discovery, communication and preservation of knowledge" [@office_of_the_president_mcmaster_university_mission_2024]; a dedication "to fostering an academic community in which the learning and scholarship of every member may flourish" [@the_office_of_the_governing_council_secretariat_institutional_1992]; by "[encouraging] and [celebrating] excellence and innovation in teaching, learning, research and student development" [@trent_university_vision_2024]; and with a determination "to put the learner at the centre of all [the university] does" [@university_of_guelph_mission_2024].

Unprincipled organizations need not question their choices of technology--as long as the technology advances whatever their goal might be at any moment. But universities aim to achieve their mission while adhering to "integrity, quality, inclusiveness and teamwork" [@office_of_the_president_mcmaster_university_mission_2024]; with "vigilant protection for individual human rights, and a resolute commitment to the principles of equal opportunity, equity and justice" [@the_office_of_the_governing_council_secretariat_institutional_1992]; and  as they "foster sustainability, in its environmental, social and economic dimensions" [@university_of_guelph_mission_2024].

Universities must strike a careful balance between what they aim to achieve and how they achieve it. And so, what do AIs do that can advance the mission of universities without compromising their core values?

## What do GenAIs do?{#what-do-ais-do}

At this point, we might as well ask "what do AIs _not_ do"? 

One reason many organizations have spent substantial resources on AIs is that they have been presented as _everything tools_: a technology capable of doing many things humans do, but faster and supposedly cheaper. While most tools are understood to have a limited range of uses, the release of GenAIs was accompanied by innumerable promises--most of them made by entities and individuals with a stake in the adoption of the technology^[Ranging from "AI is inevitable, and I will sell it to you" to "AI is inevitable, and for a consulting fee I will tell you how to level up with it".].

The list of promises have grown to encompass pretty much everything, from saving the planet [by "accelerating sustainability"; @nakagawa_accelerating_2023] to "doubling human lifespans" within the decade [@novak_anthropic_2025]. Transforming health care, finance, human resources, insurance, "and beyond" [@ibm_what_2023] pales besides such momentous accomplishments.

Colleges and universities have not been spared the onslaught of promises.

For example, @power_3_2024, presents three "innovative" uses for AI in higher education. First, handing over manual tasks to AIs could improve the budget by "releasing" administrative staff. AI could assess traits like the "grit" and "empathy" in admission applications. AI could also personalize the experience of consumers [i.e., students] by predicting their final grade with 60-70\% accuracy, and by providing corrective and instructive feedback. 

@hie_how_2023 argue that AIs will be the primary way to access information in the future. For this reason, institutions must equip students with skills like _prompt engineering_, the ability to craft effective queries for GenAIs. Universities will use AIs to improve learning and challenge students' critical thinking. Faculty will also benefit from AI by automating student evaluation and writing course syllabi. Moreover, @hie_how_2023 warn that "AI is here to stay"^[Writing in 2022, Mr. Hi√© predicted that the metaverse would be a [revolution in higher education](https://www.linkedin.com/pulse/metaverse-dedicated-pedagogy-revolution-higher-education-anthony-hi%C3%A9). This prediction aged poorly: on February, 2023, Microsoft [disbanded](https://www.theinformation.com/articles/microsoft-kills-its-industrial-metaverse-team-after-4-months) its Industrial Metaverse Core Team scantly four months after launching it. Shortly after that, Meta [slashed its own metaverse team](https://www.reuters.com/technology/meta-lay-off-employees-metaverse-silicon-unit-wednesday-2023-10-03/) in October, 2023.], and that "[faculty] might not be able to stop the AI trend from growing" so they should at least try to shape it as best they can.

Similarly hyperbolic claims about AI in higher education can be found peppered throughout the landscape [e.g., @adiguzel_revolutionizing_2023; @contact_north_five_2024; @dwivedi_opinion_2023; @johnson_ai_2023; ]. A fairly comprehensive list [from @johnson_ai_2023] is as follows (notice the use of "can" in the original):

- AI can provide personalized learning.  
- AI-powered systems can provide smart tutoring.  
- AI can automate grading.  
- AI can enable virtual classrooms.  
- AI can provide data insights for informed decisions.  
- Natural Language Processing (NLP) can enhance language learning.  
- AI can create interactive and adaptive learning materials.  
- AI can identify early signs of learning difficulties.  
- AI can aid in providing personalised professional development opportunities for teachers.
- AI can streamline administrative tasks.

Is it really possible for AIs to do all this?

## A note on the terms of the conversation {#terms-of-debate}

Presenting AI as an _everything tool_ is purposeful. This device makes debates around the technology quickly devolve into a [Gish Gallop](https://en.wikipedia.org/wiki/Gish_gallop), a rhetorical style where a party makes an overwhelming number of arguments with little regard to their accuracy or strength. To be sure, AIs can do _some_ things well, but critics are forced to counter every suspect promise with evidence, a more time consuming task than making unsubstantiated and/or misleading claims. Proponents of AI can always admit that AIs may not actually do _that one thing_, but what about the myriad of other things that people say they really do? A second discursive device deployed by proponents of AI is an informal fallacy known as [The False Dilemma](https://en.wikipedia.org/wiki/False_dilemma): adopt AI or fail to level up.

It is not my intention to confront every single possible use of AI in teaching and learning, since doing so plays into the intent of the Gish Gallop. As well, I reject the false dilemma, and remain convinced that there are multiple ways to excel at teaching and learning that do not require AI, let alone submitting to the compromises demanded by AI.

## But what do GenAIs really do? {#really-what-do-they-do}

GenAIs are _deep learning models_ [@ibm_what_2021], so-called because they are neural networks with _depth_, meaning _many_ hidden layers. Large Language Models are models built with multiple layers of neural networks trained on _very_ large amounts of textual data [@ibm_what_2023].

Behind all the jargon, though, the neural networks that underpin GenAI are models designed to produce "statistically probable outputs" when prompted [@ibm_what_2021]. In more conventional terms, neural networks are regression models--much more sophisticated and powerful than linear regression to be sure--but regression models nonetheless [@ripley_neural_1994]. Here, it is important to note that the term regression refers to [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean)--regressing an input (e.g., a prompt) to a "statistically probable" outcome.

A regression model uses as inputs a "dependent" variable $y$ (observations of an outcome of interest) and "independent" variables $x$ (observations of things thought to correlate with the dependent variable) to find a conditional mean that becomes the output of the model, or $\hat{y}$ (i.e., a mean value conditional on the values of the dependent variables). The process of finding that conditional mean is to satisfy some criterion (e.g., that the conditional mean minimizes the error of the model).

@fig-bivariate-model is an example of the simplest regression model with only one independent variable. The plot shows the pairs of $y$ and $x$ values, and the line is the model, i.e., the conditional mean $\hat{y}$. This model needs only two parameters: an intercept (the conditional mean $\hat{y}$ when $x = 0$), and the slope of the line (the rate of change of $\hat{y}$ with respect to $x$). Each additional independent variable uses one additional parameter to describe the slope of the model with respect to that variable.

```{r simulate-simple-example}
set.seed(13253)

b0 <- 1.0
b1 <- 1.5

df <- data.frame(x = b0 + b1 * runif(100, 
                                0, 
                                10),
                 e = rnorm(100),
                 type = "training data") |>
  mutate(y = x + e)

x <- df |> pull(x)
y <- df |> pull(y)

mod <- lm(y ~ x, 
          data = df)
```

```{r plot-simulated-data, message=FALSE, out.width="70%", fig.cap="Bivariate Regression Model"}
#| label: fig-bivariate-model

ggplot(df,
       aes(x = x,
           y = y)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()
```

```{r data-vs-predictions-comparison, out.width="70%", fig.cap="Training data and model output compared"}
#| label: fig-data-vs-preds-01

df_pred <- data.frame(x = x)

y_hat <- predict(mod, df_pred)

df_pred_lr <- df_pred |> 
  mutate(y = y_hat,
         type = "regression output")

df_pred_all <- rbind(df |>
                   select(x, y, type),
                 df_pred_lr |>
                   select(x, y, type))

ggplot() + 
  geom_point(data = df_pred_all,
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Neural networks do something similar but using a net-like architecture. For instance, @fig-nn1 shows a relatively simple neural network trained using using the same data as above. This network is not "deep", as it has only one "hidden layer" (the two neurons between the input and the output layers). In comparison to the linear regression and its two parameters, this neural network uses seven parameters that need to be pre-loaded with random values, which makes the results of the model contingent on the starting conditions of the training process.
```{r}
# Given the dataframe grid.1, let us now train a neural network. We need to first define a formula that specifies the dependent variable and the independent variables.
f <- as.formula("y ~ x")
```

```{r train-shallow-neural-network}
# Train a neural network using only two neurons in one hidden layer. The argument `linear.output` is set to TRUE, so that the activation function is not applied to the output node. The default activation function is the logistic function.
set.seed(33225433)

nn1 <- neuralnet(f, 
                data = df, 
                hidden = c(2), 
                linear.output = T) 
```

```{r save-shallow-neural-network}
# For some reason, the figure cannot be saved directly, so I saved it manually using screen capture
plot(nn1)
```

```{r plot-shallow-neural-network, out.width="70%", fig.cap="Simple Example of a Shallow Neural Network"}
#| label: fig-nn1

knitr::include_graphics(glue::glue(here::here(), "/images/nn1.png"))
```

```{r shallow-neural-network-predictions}
y_hat <- predict(nn1,
                 df_pred)

df_pred_nn1 <- df_pred |> 
  mutate(y = y_hat,
         type = "nn1 output")

df_pred_all <- rbind(df_pred_all |>
                   select(x, y, type),
                 df_pred_nn1 |>
                   select(x, y, type))
```

Unlike a linear regression model, the predictions of the neural network can be non-linear (@fig-data-vs-preds-02). This flexibility is achieved by being a less parsimonious model (it uses more parameters).  

```{r data-vs-predictions-comparison-2, out.width="70%", fig.cap="Training data and model outputs compared: linear regression and shallow neural network"}
#| label: fig-data-vs-preds-02

ggplot() + 
  geom_point(data = df_pred_all,
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

More complex neural networks can be designed. @fig-nn2 shows a deeper network, with two hidden layers of five neurons each. This model uses forty-six parameters. 
```{r train-deeper-neural-network}
# Train a neural network using only two neurons in one hidden layer. The argument `linear.output` is set to TRUE, so that the activation function is not applied to the output node. The default activation function is the logistic function.
set.seed(242)
nn2 <- neuralnet(f, 
                data = df, 
                hidden = c(5, 5), 
                linear.output = T)
```

```{r save-plot-deeper-neural-network}
# For some reason, the figure cannot be saved directly, so I saved it manually using screen capture
plot(nn2)
```

```{r plot-deeper-neural-network, out.width="70%", fig.cap="Example of a Deeper Neural Network"}
#| label: fig-nn2
 
knitr::include_graphics(glue::glue(here::here(), "/images/nn2.png"))
```

```{r}
y_hat <- predict(nn2,
                 df_pred)

df_pred_nn2 <- df_pred |> 
  mutate(y = y_hat,
         type = "nn2 output")

df_pred_all <- rbind(df_pred_all |>
                   select(x, y, type),
                 df_pred_nn2 |>
                   select(x, y, type))


```

The increased complexity leads to greater flexibility, and we can see that the model's predictions resemble more closely the actual data (@fig-data-vs-preds-03).  

```{r data-vs-predictions-comparison-3, out.width="70%", fig.cap="Training data and model outputs compared: shallow neural network and deeper neural network"}
#| label: fig-data-vs-preds-03

ggplot() + 
  geom_point(data = df_pred_all |>
               filter(type != "regression output"),
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r}
# How do these models perform?
mad_lr <- data.frame(df |>
             mutate(y_hat = df_pred_lr$y)) |>
  summarize(mse = sum(abs(y - y_hat)/n()))

mad_nn1 <- data.frame(df |>
             mutate(y_hat = df_pred_nn1$y)) |>
  summarize(mad = sum(abs(y - y_hat)/n()))

mad_nn2 <- data.frame(df |>
             mutate(y_hat = df_pred_nn2$y)) |>
  summarize(mad = sum(abs(y - y_hat)/n()))

delta_mad_nn1 <- (mad_nn1 - mad_lr)/mad_lr * 100

delta_mad_nn2 <- (mad_nn2 - mad_lr)/mad_lr * 100
```

Indeed, the first neural network is "better" than the linear regression in that it reduces the typical error by `r paste0(prettyNum(abs(delta_mad_nn1), digits = 2), "%")`, whereas the deeper network reduces the typical error by `r paste0(prettyNum(abs(delta_mad_nn2), digits = 2), "%")`, also with respect to the linear regression^[The "typical error" here is the mean absolute deviation of the predictions.].

There is a limit as to how flexible a regression model can be. Each additional parameter makes the model more flexible, but as a rule we cannot use more parameters than the number of data points that exist in the dataset used to train the model. Roughly, every additional parameter needs one additional datum to train on, and when there are as many parameters as data inputs the model becomes overfit and predicts each value of the independent variable in the training dataset perfectly. The model will fail to train if there are more parameters than data points.

In a neural network trained to work with natural language, the "predictors" are the words that come before and after a certain word. Several approaches exist to _embed_ a "bag" of words, which means using massive amounts of text to calculate how frequently words go together. These frequencies are then converted to numerical vectors that represent the "position" of a word in multidimensional space, and then the "distances" between words.

Here are some key concepts to keep in mind when discussing AIs:

- AIs are neural networks, which is to say, regression models.
- Regression models generate outputs that are regressions to the mean.
- AIs are _very_ flexible non-linear regression models thanks to the use of "deep" architectures, which means many layers with many parameters^[According to reports [see @griffith_gpt_4_2023], GPT-1 had 117 million parameters, GPT-2 1.5 billion, and GPT-3 175 billion parameters. OpenAI no longer reports the number of parameters of its newer models, but in the latest version this is estimated to be in the trillions.]. 
- The large number of parameters makes these models _extremely_ data hungry. 
- AIs are trained using an initial set of random values. The same training dataset can result in different models due to the randomness in the initial conditions.
- The flexibility of AIs means that outputs will vary seemingly at random with relatively small changes in the prompt. 
- Also, importantly, since regression models aim for the mean (i.e., a statistically probable output), they tend to be bad at _extrapolation_ (or thinking outside the box, if AIs could think). The further away an input (i.e., prompt) is from the space defined by the training dataset, the worse the output will tend to be^[Which is why so-called hallucinations became more subtle as bigger datasets were used to train AIs. The increasingly large datasets essentially expanded the space where models could interpolate.].
- However, when data around a topic are sparse, the regression model can output exactly the training data, because the correlation is perfect.

## GenAIs in teaching and learning {#ai-in-teaching-and-learning}

Uses of AI in universities include writing text (e.g. reference letters, grant applications, job descriptions, report summaries), conducting qualitative and quantitative data analysis, translation, captioning, providing multiple formats for learning and engagement, and for personalized support. Anecdotally, students report wanting to use generative AI not to complete assignments for them, but to help enhance learning in their courses by having concepts explained differently, by working on practice questions or by getting immediate feedback on drafted work. In short, AIs are supposed to help both teachers and students.

### Different explanations of concepts

There are many ways for students to get different perspectives on concepts. They can form study groups, attend office hours to engage with their instructors, ask their Teaching Assistants, ask relatives and friends, and so on. But even for a student who for whatever reason cannot or will not do any of these things, the fact remains that relatively common concepts have already been explained by humans in dozens if not hundreds of different ways, often in writing that is available through their university library. Is an AI tool essential to find alternative explanations for common concepts? Only as a shortcut for a good library search (or to make up for the lack of an academic network, not taking advantage of office hours, etc.), and at the expense of research skills. As for more obscure concepts, given their paucity in training datasets, those would tend to be uncommon, and therefore improbable outputs (i.e. for reasons of lying far from the mean). This strongly suggests that any outputs about specialized queries should be treated with caution.

### Practice questions 

Most textbooks already include plenty of practice questions, especially for relatively common topics. For more specialized topics, asking interesting questions should be a skill in and of itself. An emphasis on practice questions, however, signals an underlying weakness of the process, namely, a focus on learning for the examination, as opposed to learning for its own sake. Posing interesting questions is an essential part of an instructor's job, but this part of the job becomes compromised when instructors are pushed to operate in massive classrooms. In this case, AI is a substitute, and demonstrably not a good one, for a qualified instructor. At issue, at well, is traditional evaluation approaches. A growing body of evidence points towards the effectiveness of alternative assessment schemes, such as continuous assessment through course work, and even seen examinations instead of the traditional unseen examinations [@turner_see_2018]. Alas, unseen examinations continue to dominate the evaluation landscape [@buckley_are_2023]. Ungrading, in particular, is increasingly recognized for its potential to help learners internalize the motivation to learn, and for fostering adaptability, creative thinking, and self-management [@gorichanaz_it_2024]. If studying for the examination often detracts from genuine learning, why are other assessment approaches seldom considered? Research by @vahed_continuous_2023 suggests that instructors need to develop assessment literacy, that is, an understanding of "the fundamental assessment concepts and procedures, and to engage in the ongoing process of adjusting instructional strategies to enrich students learning experiences". There is a indeed a dire need for innovative approaches in assessment, but it is unlikely that AI is the tool to develop them.

### Immediate feedback on drafted work

This is an utterly unrealistic expectation. Ideally, students should have the opportunity to discuss their drafted work with instructors before final submission (or could ask members of their study groups to proof-read their work, among many other ways to get feedback before submission). A conscientious instructor will invest time reading the drafted work to provide quality feedback. There is, however, a more insidious aspect about getting feedback from an AI...who, actually, provides the feedback in this case? Whose data are used to train the machine that provides the feedback? Who fine-tunes the machine that provides the feedback? Whose perspectives and views are used to provide the feedback, and can they ever be accountable for the feedback provided? A poignant example of this dilemma involves AI-based face recognition and online proctoring, as related by @hill_accused_2022.

### Creation of innovative learning materials

@mollick_instructors_2024 envision a future in which educators are innovators thanks to their adroit use of prompts for AIs. According to these authors, GenAI stands alone among educational technologies in that it can be "'programmed' through prompts alone", which should allow even instructors "without extensive technology or coding experience" to more easily create classroom applications. 

Leaving the technology fetish about classroom applications aside, Mollick and Mollick are well aware of the ethical concerns around using AI (see p. 6), including that "[m]odels are trained in ways that may violate copyright", that exploit the "efforts of low-wage workers in precarious conditions for part of their training", and the fact that "[m]odels are trained on biased data and can produce either subtly or overtly biased results". And yet, they choose to ignore every single one of these concerns for the sake of innovation. Machine-washed plagiarism that exploits low-wage workers while using inordinate amounts of energy is innovative only in a way that offends "integrity" [@office_of_the_president_mcmaster_university_mission_2024], "individual human rights...equity and justice" [@the_office_of_the_governing_council_secretariat_institutional_1992], and "sustainability, in its environmental, social and economic dimensions" [@university_of_guelph_mission_2024].

But what if the datasets and training processes used to develop GenAIs were unimpeachable, would the practice of generating learning materials using AIs then be "innovative"? Alas, no. As should be clear by now, any teaching materials generated in this way can only aspire to be average given the prompts used to get the machine going, and no one should buy the delusion that the use of this technology makes them innovators. In any case, the list of remedial measures required to verify that the materials generated are not nonsense is so long, that it is hard to see what is being gained [see @mollick_instructors_2024, pp. 4-5].

### Automated grading

There is nothing new about automated grading: [Scantron](https://www.scantron.com/company/) sheets for multiple choice examinations have existed for over four decades. They work well because the algorithm needed for grading multiple choices is simple, and importantly, non-random. There is no reason to use a neural network to grade multiple choice examinations. But on the flip side, there is also no evidence that a neural network can perform well at grading anything more complex than a multiple choice examination, since 1) it will tend to consider things that are closer to the mean as "better"; and 2) the amount of training data required for a classification task of this nature is massive. Labeling data is an essential, yet extremely expensive aspect of training GenAIs. The algorithm needs a "dependent" variable for training, a "label", which in this case a grade that a human assigned to a piece of work. Given all the possible variations of assignments (by year, by subject, by instructor), a general grading machine would require vast volumes of _labelled_ training data, and even so there is no guarantee that the results would be adequate, let alone appropriate.

### Writing reference letters

Using a chatbot to write reference letters must be one of the most pointless uses of chatbots, or one of the most dishonest. Imagine that the reference letter honestly included a disclosure that ChatGPT was used to write it. If the objective of a reference letter is for someone to attest from the vantage point of some level of expertise as to the skills and qualities of a person, a chatbot-written letter is probably worse than useless: what message does the disclaimer send to the recipient of the letter? And how much stock will anyone place on the veracity of the letter? To avoid this catch, one would have to occult or obscure the use of AI, an act that lays bare the dishonesty of the activity.

### Qualitative and quantitative data analysis

Computers have been doing mathematics reliably since at least the [Antikythera mechanism](https://en.wikipedia.org/wiki/Antikythera_mechanism) (ca. 178 BC),. Electronic computers have been doing mathematics for decades now, and neural networks have been used for decades to do data analysis. There is nothing new about that.  But the promise rather is that we will be able to do data analysis without having to _learn_ how to do data analysis. The idea that a chatbot could be used to do data analysis betrays a profound misunderstanding of both what chatbots do and what data analysis entails. Ask a chatbot to do some arithmetic: it will produce something that is right on average. But average is not always right. And seeing how AIs are now being trained on the dregs of the internet, it should be no surprise when an AI Overview responds to the prompt "2 + 2" with "potato".

```{r flickr-potato, out.width="70%", fig.cap="Data analysis using a chatbot? Maybe not such a good idea"}
knitr::include_graphics(glue::glue(here::here(), "/images/potato.png"))
```

An error like this is easy to spot. But we do not use computers to do calculations that can be done by hand, and checking the correctedness of an AI-generated response is as time consuming as doing the analysis without an AI [see @yuan_how_2023].

From a different perspective, even the average output of an AI should be treated with caution. As noted [above](#really-what-do-they-do), the results of a neural network depend on the weights selected at random at the start of a training process. Change those weights, the results will change. As well, even small variations in the prompt can be impactful in models with billions or trillions of parameters. In statistical terms, any output is but one _realization_ of a random process. For the results to be useful, we should calculate the average of a large number of responses. Given the cost of training and querying LLMs, this is prohibitively expensive. 

But what about a task that involves logic instead of arithmetic? Research by @nezhurina_alice_2024 demonstrates that relatively simple natural language puzzles that are easily solved by humans often result in confidently expressed errors when used to prompt an AI. Wrong answers, furthermore, are frequently backed up by confabulations, that is, nonsense expressed in reasoning-like terms. The connections between words that LLMs deliver are but probabilistic word salads: @xu_hallucination_2024 use a formal approach to show that LLMs are inherently incapable of mapping outputs to truths, and thus their results will only ever resemble truth purely by chance. The outputs of a chatbot only sound plausible thanks to the vast amount of data points and parameters that the models use. But if the objective of qualitative research is to identify interesting aspects of the human experience, it is certainly not the case that they will be found in a regression to the mean.

### Identify early signs of learning difficulties.  

An AI should not be used to identify signs of learning difficulties for the same reasons that AIs cannot be trusted to identify text written by an AI. The false positive and negative rates are disproportionate compared to the benefits. A vigilant instructor will be aware of learning difficulties among their students. But were the process to become automated, who would decide when an intervention was warranted, and who qualified for it? Would the intervention be offered to everyone flagged by the system, including the false positives? And, could those denied an intervention complain? Importantly, who would fund an intervention, and who would deliver it? Another AI? 

### Personalized learning

Anyone receiving "personalized learning opportunities" from an AI can be sure that they will be treated as the machine-generated average of the dataset used to train the model, and not as a person.

## What do GenAIs do to students and teachers? {#what-can-ais-do-to-students}

The preceding section illustrates numerous AI tasks in higher education are in fact poor replacements for a human-centered education. But what about the things that GenAIs can do _to_ students?

### Distract from learning and deskilling

Reliance on AI short-circuits the learning process. Statements like "in the very near future AIs will likely be the primary way we access knowledge" [@hie_how_2023] imply that prompt engineering will become an essential skill, perhaps even the skill to end all skills^[Prompt engineering is a pompous name for a task that more closely resembles reading the entrails of a Large Language Model than actual engineering. It is a task that cannot be performed in a systematic way, and that offers no guarantees that it will work in the same way again, between models or versions. Calling the activity of writing prompts "engineering" is to steal the competence and reputation of actual engineering.].

It is important to note that, even if prompt "engineering" offered some value, the skill is tied to a singular technology that offers little transferability potential. @mollick_instructors_2024 demonstrate how prompt "engineering" is not even transferable _between_ different LLMs (see p. 5 and p. 36). Good skills transfer between contexts. Learning to do effective bibliographical queries transfers between libraries. Learning to identify relevant content works for reading a novel, a book of history, or a text of econometrics. Writing, the slow and demanding process of converting thoughts into words, is essential to create a style of communication that is unique to each person. It might be argued, in fact, that writing _is_ learning [@klein_trends_2016].

An AI is like a crutch, but what we are being promised is an All-Terrain Vehicle, however one that the user will never own, and cannot hope to fully master due to its blackbox nature (neural networks are blackboxes and their development is also a blackbox wherein things can more easily change based on the interests of their corporate owners than in response to the interests of their relatively small customers^[And this refers not only to individuals. As an example, in 2022 McMaster University, a major Canadian university, operated on revenues of less than [\$843 million](https://financial-affairs.mcmaster.ca/app/uploads/2023/06/2023-24-Consolidated-Budget-FINAL.pdf) (CAD). Microsoft, in contrast, operated on revenues of approximately [\$198,000 millions](https://www.microsoft.com/investor/reports/ar22/) (USD) in 2022. To put things in perspective (and using the average exchange rate of 2022), if McMaster used _all_ of its 2022 revenue to pay Microsoft, that would leave McMaster with zero dollars and still account for less than _one third of one percent point_ of Microsoft's revenue.].

<!--
McMaster: 842,899,000 CAD
Microsoft: 198,000,000,000 USD == 257,776,200,000 CAD

https://www.exchange-rates.org/exchange-rate-history/usd-cad-2022
Avg. exchange rate in 2022: 1.3019 CAD/US

This is how much bigger Microsoft is compared to McMaster. 
(198000000000 * 1.3019)/842899000 

Put another way: If McMaster gave ALL OF ITS REVENUE to Microsoft, it would only account for 842899000 / (198000000000 * 1.3019) or less than one third of a percentage point of Microsoft's revenue.
--->

However, let us assume for a moment that using AIs in their current form does becomes an in-demand tool in the short term. What is the expected longevity of a skill like prompt "engineering"?

Improvement in the performance of AIs has relied on using bigger datasets. This has led to a rush, as developers train their models using bigger datasets^[A rush that feels as frenetic as any gold rush in the past. However, while the gold rushes of the 19th century were relatively democratic due to their low entry costs, the current data rush has enormous entry costs and is dominated by large corporations and venture capital.]. But, despite the large number of text items and images available in digital form, the supply is finite.

As of this writing, the few big players that can afford to grab the data, are mostly done with it, using morally dubious and possibly illegal means [@clark_ex-amazon_2024; @milmo_zuckerberg_2025; @smalley_linkedin_2025]. And so, as the fountain of human-generated content has been reduced to a drip, there has been a turn towards what might be the last untapped drops of information left on the internet.

OpenAI, a global leader in data grabbing, resorted to data from Stack Overflow [@stack_overflow_stack_2024], a website that [bills itself](https://stackoverflow.com) as "[a] community-based space to find and contribute answers to technical challenges". Said community was [unhappy](https://favtutor.com/articles/stack-overflow-community-not-happy-openai/) with the deal, and has come to believe that the terms were abusive: while Stack Overflow's terms of service state that posters cannot revoke permission to use, the site also operates under a Creative Commons license that requires attribution, something that AIs notoriously _do not_ and _cannot_ do. It is an open question how long Stack Overflow will remain a community-based space after it started treating its members as unwitting, and even unwilling providers of data for OpenAI [@grimm_stack_2024].

Google, for its part, appealed to Reddit for training data to the tune of \$60 million per year [@roth_google_2024]. As Reddit content begins to creep into Google's AI-assisted search, the results of this deal have turned grotesque, with reports of AI Overview failures that include a suggestion to put non-toxic glue to pizza sauce to keep the cheese from sliding [@koebler__google_2024].

```{r google-reddit, out.width="70%", fig.cap="Google's AI Overview result was traced back to a post by redditor Fucksmith"}
knitr::include_graphics(glue::glue(here::here(), "/images/google-cheese-glue.png"))
```

Other AI Overview results included a recommendation by geologists to eat at least one small rock a day, a piece of advice tracked back to an article published by the satirical periodical [The Onion](https://www.theonion.com/geologists-recommend-eating-at-least-one-small-rock-per-1846655112).

```{r google-the-onion, out.width="70%", fig.cap="Google's AI Overview result was traced back to an article in The Onion"}
knitr::include_graphics(glue::glue(here::here(), "/images/google-the-rock.jpg"))
```

These, and numerous other hapless results, are produced by an AI that Google has already spent tens of billions of dollars developing [@vynck_after_2024]. To what extent can more money improve GenAIs? Probably not by much, since a key constraint is the amount of data available for model training [@villalobos_position_2024]. And dismal as this is, the outlook may be even worse than simply a frantic race after more human-generated data. As GenAIs become more widely used, they can produce data at a pace that humans cannot match, much of which is already finding its way into the internet in the form of text and images--and not only through clickbait websites and disinformation [@ruffo_studying_2023], but also in scientific writing, supposedly the world's most thoroughly vetted form of writing [@cabanac_tortured_2021; @maiberg_ai_2024].

What can we expect as future GenAI models begin to be trained using data that was generated by previous GenAIs? Revisiting the examples in a preceding section ([But really, what do AIs do?](#really-what-do-they-do)) we can see that the predictions of a model are more uniform than the original data used for training: the outputs of a model will be less varied, less diverse, and more norm-conforming. The implication should be clear: training models with the output of models is a recipe for increasingly bland outputs. We may be witnessing _Peak ChatGPT_, and future model versions, instead of improving, may begin to regress to duller versions of their own previous outputs. To compound things, as the backlash against an exploitative data rush is suuported by new tools to generate data that are deleterious to the training process of a GenAIs [@salman_raising_2023], including _PhotoGuard_ [@heikkila_this_2023], _Nightshade_ [@shan_nightshade_2024], and _Shadowcast_ [@xu_shadowcast_2024].

All in all, the evidence suggests that the scarcity of fresh data, paired with more abundant model-generated and/or poisoned data, will make the performance of these machines stagnate or deteriorate. Which begs the following questions. How useful will AIs be in the future? How persistent will skills like prompt "engineering" be? Will learning how to prompt an AI be worth sacrificing the opportunity to develop transferable skills like coding, writing, and thinking? Who, after all, stands to benefit the most by the form of deskilling that AIs encourage?

## What do GenAIs do for the mega-corporations pushing them? {#what-do-ais-for-corporations}

AIs do not seem to do much of value for students and teachers, as they are wasteful at best and harmful at worst. But the technology must do something for someone, otherwise it would not be ubiquitous. So what do AIs do for the the organizations that develop them? So far, the breakneck race to develop AI has soaked immense amounts of venture capital and other speculative money, in addition to incalculable resources by many other entities^[Some observers believe that the AI fever has become detrimental to actual innovation [@siegel_ai_2023; @vinsel_dont_2023].]. Between 2012 and 2020, venture capital investments in AI grew twenty five-fold, from \$3 billion (USD) to \$75 billion (USD)^[For perspective, Ontario's Ministry of Colleges and Universities, whose mission includes supporting "research and innovation to help the province compete and thrive in the global economy", spent $163,482,281 (CAD) on its research program in 2021-2022 [@ontario_ministry_of_colleges_and_universities_expenditure_2023], a mere blip (~0.0017\%) that barely registers compared to venture capital investments in AI in 2020 alone.] [@tricot_venture_2021]. 

Alas, the industry has not necessarily made money for investors yet. 

@berber_peter_2024, for instance, reports that the AI industry spent \$50 billion (USD) in 2023 on chips alone, while bringing in only $3 billion in revenue. @patel_inference_2023 estimated that incorporating ChatGPT-like LLMs into search would lead to a loss of \$30 billion (USD) in profit for Google. At a guess, the industry is running at a loss in almost every respect: in the same report, @patel_inference_2023 calculated the cost of running a query in ChatGPT at about \$0.36--so it would take just about 57 monthly queries per user with a "Plus" subscription ([\$20/month](https://openai.com/chatgpt/pricing/)) for OpenAI to begin losing money on that one customer. And a large number of users do not pay: based on revenue reports [see @perez_chatgpts_2024] there were at most 210,000 subscribers to the "Plus" plan in 2024, out of a user base estimated at tens of millions [@tong_exclusive_2023]. The profitability of AI would look even worse if not for the fact that the cost of labor is suppressed by outsourcing  to the Global South the most disturbing, degrading, and damaging aspects of training AIs [@nix_analysis_2024].

Venture capital is not known for its charitable nature, though, and at some point investors will want their money back with a heap of profit on top [@sriram_ai_2023]. Who will pay to make investors whole? And how much those who end up paying will need to pay? Or, in the worst case, will _have_ to pay, if their skills become tied to this one technology? Deskilling, both of individuals and of organizations, appears to be one of the few pathways to AI profitability: once people have failed to learn a skill, and therefore become dependent on the tool, it becomes possible for the maker of the tool to turn into a digital rentier [@sadowski_internet_2020; @komljenovic_rise_2021]. From this perspective, what is on offer by the AI industry is SaaS--Skill as a Service.

## What do GenAIs do to everyone and everything else? {#what-do-ais-do-to-everyone-else}

### AI is an eminently extractive and exploitative industry

ChatGPT 4.o was announced on May 13, 2024, with great fanfare and a flashy display of the model's new capabilities, including functions to respond vocally to prompts, and to identify images via a mobile device's camera. 4.o's voice, in particular, caused considerable titillation [@roose_is_2024], modeled as it was after the digital personal assistant of the movie _Her_ (Sam Altman's favorite movie by his own account). The background story of 4.o's voice is more sordid: Altman reportedly tried to hire Scarlett Johannson, who played the AI assistant in the movie, to be the voice of ChatGPT 4.o [@kastrenakes_scarlett_2024], but when she declined, OpenAI went and used a mimicry of [her voice](@as-an-aside) anyways. This voice was retired after Ms. Johansson threatened legal action [@allyn_scarlett_2024]. 

Alas, very few have the wealth and social standing of Scarlett Johansson to challenge OpenAI. Most people in the world lack that power and the AI industry has not been shy to exploit the imbalance [@roberts_commercial_2016; @germain_google_2023; @weatherbed_adobes_2024]. 

Besides being extractive, the AI industry is exploitative in darker ways. ChatGPT was trained on vast amounts of data scooped from the internet, where many societal problems are amplified. Trained on horrible content, it was hard to sell a ChatGPT prone to racist, sexist, obscene outbursts. Fortunately for OpenAI, companies like facebook had already hit on a solution of sorts to reduce the cost of moderation. The idea was basically to train AIs to detect content that was unpalatable. Indeed, they had found that feeding an AI with labeled examples of all the worst that exists on the internet (e.g., violence, hate speech, child sexual abuse) could produce tools that learned to detect various manifestations of such toxicity, to filter it out before it reached users. To obtain the essential labeled inputs, OpenAI sent tens of thousands of samples to Kenya, with text that described murder, suicide, bestiality, self-harm, incest, torture, and child sexual abuse situations in graphic detail. A sanitized ChatGPT is only possible through the labor (with wages around $1.32 and $2 per hour) of workers who were exposed to the worst of the internet. As one worker tasked with reading and labeling text for OpenAI told TIME, "he suffered from recurring visions after reading a graphic description of a man having sex with a dog in the presence of a young child. 'That was torture,' he said. 'You will read a number of statements like that all through the week. By the time it gets to Friday, you are disturbed from thinking through that picture'" [@perrigo_2_2023].

### Privacy issues

LLM are known to regurgitate verbatim chunks of the training datasets. This is called _memorization_, or in more conventional terms, model overfitting--an extreme regression to the mean that excels at predicting data in the training set. Overfitting happens when a model has too many parameters or too few data inputs. Data "memorization"--the verbatim or quasi-verbatim output of training data--can be as high as 7% in the case of some LLMs [@peng_near_duplicate_2023, p. 8]. This means that data cannot be presumed to be private once that they have been used to train an AI.  And there is besides the issue of data leaks. So far, organizations such as the National Archives and Records Administration in the US, the US Congress, and companies like Samsung and Apple have restricted or banned the use of ChatGPT and Copilot for data security reasons [@koebler_national_2024; @singh_after_2024]: no data transferred in the form of a prompt can be presumed to be safe.

### Environmental impacts

Every human activity has an environmental impact, and at a time when we face unprecedented challenges with climate change, we must be discerning in how we consume resources. The choice is clear and Microsoft has told us: we need to burn the planet [@smith_our_2024] to create the AIs that will help us save the planet [@nakagawa_accelerating_2023].

Researchers like @crawford_generative_2024 have already sounded the alarm: the environmental costs of AI are soaring, and doing so mostly in secret [@luccioni_power_2024]. It has been estimated that ChatGPT consumes up to 25 times more energy than a regular web search,, and according to some estimates, the whole of AI may consume twice as much energy as the whole of France by 2030 [@brussels_chatgpt_2024]. As the examples in the preceding sections illustrate, this is in exchange for results that are not better than a simple search, and often are considerably worse.

### Reproduction of unjust structures of power

When using a Generative AI we must ask whose perspectives the tool will tend to reproduce and amplify. In this sense, numerous observers [e.g., @ahuja_we_2023; @stross_tech_2023; @gebru_tescreal_2024] have noted the ideological leanings of various actors in the AI scene, an elite world of tech investors with a political agenda hiding in plain sight^[@thiel_education_2009 has written about how he "no longer believe[s] that freedom and democracy are compatible". Sam Bankman-Fried's FTX donated millions to a group with racist ties [@wilson_sam_2024]. Elon Musk once shared a meme comparing Canada PM Justin Trudeau to Hitler while defending the vandals who terrorized Ottawa in 2022 [@henderson_and_elon_2022].].

Besides concern about the influence of a small group of uber-wealthy people who collectively can muster more than _half a trillion dollars_ to turn their beliefs into reality [@stross_tech_2023], there is still leaves immediate problem that the datasets used to train AIs are, like the technology itself, not politically neutral. _Platformed racism_ [@matamoros-fernandez_platformed_2017], for instance, is known to reproduce malignant biases, that can emerge in a grass-roots fashion, but are also assisted by bad-faith actors^[Russian disinformation is known to operate by seeding distrust between ethnic communities [@svedkauskas_analysis_2020]. A U.S. Department of State report notes that Russian intelligence exploits "all kinds of separatism and ethnic, social and racial conflicts, actively supporting all dissident movements--extremist, racist, and sectarian groups" to destabilize internal U.S. politics [@us_department_of_state_gec_2024, p. 51].]. Given how cagey corporations are about the data they use to train their models, it is anyone's guess how much dis- and misinformation has already been passed to their AIs^[This is part of the Unfathomable Data problem discussed by @bender_dangers_2021.].

Search engines already tended to reinforce racism [@noble_algorithms_2018]; given the above, no one should expect AI-augmented search engines to do any better^[A report by the Center for Countering Digital Hate found that Google's Bard generated wrong and/or hateful outputs in 78 out of 100 tests [@center_for_countering_digital_hate_googles_2023]. In addition, AI is also a tool ripe for abuse. Google's own researchers have come to the inevitable conclusion that it is easier to produce disinformation with AI, than it is to use AI to combat it [@maiberg__google_2024].]. In another wretched incident, Google's AI Overview stated that Barack Obama was the first Muslim president of the US, a regrettable piece of misinformation that follows on a years-long Russian-assisted right-wing propaganda campaign to otherize Mr. Obama [@tenbarge_glue_2024]. 

Mishaps like this, and others, are dismissed by Google as attempts "to trip up the technology with uncommon questions" [@tenbarge_glue_2024]. This dismissal only reinforces the notion that AIs are only ever useful when [regressing to the mean](#really-what-do-they-do). Responses like fiddling with the model (fine tuning it) to provide more palatable outputs^[Like in the case of [Google's response](https://blog.google/products/search/ai-overviews-update-may-2024/) to the poor quality of its recently launched AI overviews.] are not reassuring. Who is to say what a handful of unaccountable corporations will find unpalatable tomorrow given their insatiable quest for profits, let alone in the face of demands by powerful authoritarian regimes?

## Concluding remarks

Neural networks are nothing new and they have their uses. But those uses have been suffocated, and to some extent discredited, by the current, carefully manufactured hype around Generative AI [@morrone_signals_2024]. There are now calls to press pause on the hype [e.g., @angwin_press_2024] and universities and colleges would be wise to heed this advice, lest they fall prey to a confidence trick. Higher education is often driven by a deeply ingrained belief in innovation (even, or perhaps particularly, for its own sake), and this impulse is compounded by how AI adoption has been drawn as a false dilemma,  triggering a fear of missing out. Other pressures at work include a decades-long history of manufactured crises in education [@cizek_give_1999; @usher_state_2023].

This essay described the numerous issues with AI as an _everything tool_.

Current AI technologies cannot be used in a way that is consistent with values like integrity, respect for human rights, and environmental sustainability. This is well-known: most discussions of AI in education acknowledge that their use is rife with problems. For instance, @mollick_instructors_2024 note issues with copyright, exploitation, reproduction of biases, automation bias, and privacy loss. These are issues that "educators may want to consider before deciding whether to use these systems". This disclaimer highlights the important role of institutions to provide the moral and ethical clarity that may elude individual educators working under desperately under-resourced conditions, or simply willing to win a race to the bottom. And yet, I have little illusion that these arguments will prove persuasive. 

I would still appeal, though, to the university's sense of self-regard and self-preservation. 
We would do our students an unforgivable disservice by offering them a skill that regressed them to mediocrity and left them at the mercy of predatory corporations. "This university uses ChatGPT to teach" is simply a way of saying that we are too cheap to offer a human-centered education. But in truth, a university is too expensive an institution to be attractive under such conditions, when there are far less expensive alternatives for an AI-based education ("for only $9.99 proof yourself against layoffs"). Following the hype would be a regrettable mistake at a time when universities are under siege from unsympathetic or actively hostile actors. A university‚Äôs main source of authority is its reputation as a place that preserves and expands knowledge in a principled way. If we give this principle away, what would be left to sustain universities as institutions that add value to society?

## References
