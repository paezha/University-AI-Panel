---
title: "An AI is Haunting Campus^[Karl Marx wrote about a \"spectre...haunting Europe--the spectre of communism\"; GenAI looks awfully like the spectre of unregulated surveillance capitalism.]: What Role for the Human in Higher Education?"
format: docx
bibliography:
  - "`r system('kpsewhich ../bibliography/bibliography.bib', intern=TRUE)`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)
```

```{r include=FALSE}
library(cowplot) # Streamlined Plot Theme and Plot Annotations for 'ggplot2'
library(dplyr) # A Grammar of Data Manipulation
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(knitr)
library(magick) # Advanced Graphics and Image-Processing in R
library(neuralnet) # Training of Neural Networks
```

## Abstract

The use of Generative Artificial Intelligence (AI) in higher education has been a sensational topic since the launch of ChatGPT and similar tools. AI has been presented as an everything tool that can do a fabulous array of tasks, from creating content to grading to providing feedback to students. This presentation of AI is misleading, and there is a need to examine the claims about what the technology can do and what it actually does in the context of teaching and learning. In this essay I scan the academic and gray literature, along with media reports regarding AI, to show how discussions about AI hinge on making a large number of unverified claims (Gish Gallop) while presenting the technology as inevitable (a False Dilemma). However, once we examine what AI does, and particularly what it does for whom and to whom, a picture emerges of a technology that is intrinsically inimical to the principles that most institutions of higher education claim to hold dear, including the best interest of their students.

## Introduction

On November 30, 2022, OpenAI released what became its flagship product, [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT), a chatbot based on [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) designed and trained to mimic natural language. The launch of the chatbot was an impressive feat of marketing [@cowen_chatgpt_2023], and it took less than two weeks for the hype to reach stratospheric levels [@roose_brilliance_2023; @lock_what_2022]. Soon after, seemingly everyone wanted in on the Generative AI (GenAI) craze^[The pattern of cyclical hypes around AI even has a name: [AI winters](https://en.wikipedia.org/wiki/AI_winter); the current one, with chatbots playing the loudest horn atop the bandwagon, is just the most recent cycle of hype [see @siegel_ai_2023].]. 

According to some, AI technologies in education "offer new tools and applications that have the potential to transform traditional teaching and learning methods" [@adiguzel_revolutionizing_2023, p. 1], and have "the potential to fundamentally change the classroom experience and the knowledge and skills outcomes of our students" [@dwivedi_opinion_2023, p.20]. Furthermore, the technology "is here to stay...[t]herefore, teachers and students must develop the specific digital competencies needed to use such tools in ways that are pedagogically beneficial and ethical" [@kohnke_chatgpt_2023, p. 546]. 

AI was quickly painted as nothing short of revolutionary: an _innovative_ and _exciting_ technology, capable of _enhancing_ research and education, and that moreover can lead to _efficiencies_^["Efficiencies", to be clear, is the day's euphemism for cutting costs [@oconnell_let_2012, p. 64]. Cost-cutting is usually implemented by finding "redundancies" rather than finding better things for people to do.]. Institutions of higher education were compelled to get in on the act, forming high-level advisory committees and drafting guidelines to guide the university community leverage AI in principled ways [e.g., @office_of_teaching_and_learning_provisional_2023; @mcmaster_university_provisional_2024; @trent_university_generative_2024; @university_of_toronto_ai_2024].

For perspective, hammers are powerful tools, and so are thousands of other instruments found at universities. And yet not every single one commands a high-level university committee to advice on their use. Just what is it about AIs that requires pan-university consultations on how to use them? To understand why practically every institution of higher education has decided to spend considerable resources studying the adoption and responsible use of AI, one needs to understand what the technology claims to do that sets it apart from other tools--and then, examine what it actually does. 

In doing this, we must remain alert to the fact that technologies are never politically neutral, and two questions must be asked when considering their adoption:

- What does a specific technology do for whom?

- And what does it do to whom?

As well, we need to understand the character of the organization that considers the use of the technology.

This essay aims to provide some clarification around 1) claims about the potential of GenAIs in education; and 2) what GenAIs actually do--within 3) the context of a principled organization. I conclude that the promise of generative AI has been overstated, that is is unlikely to deliver, and that in any case the technology, as sold by the tech industry, is antithetical to many of the values that universities claim to hold dear.

# The university's mission {#the-mission}

Universities present themselves as institutions that serve "the social, cultural, and economic needs of [their] community and [their] society" [@office_of_the_president_mcmaster_university_mission_2024]; that strive "to make valued and socially responsible contributions to our local communities, to Canada, and to the world" [@trent_university_vision_2024]; and that "serve society and...enhance the quality of life through scholarship" [@university_of_guelph_mission_2024]. 

This goal is achieved through "the discovery, communication and preservation of knowledge" [@office_of_the_president_mcmaster_university_mission_2024]; a dedication "to fostering an academic community in which the learning and scholarship of every member may flourish" [@the_office_of_the_governing_council_secretariat_institutional_1992]; by "[encouraging] and [celebrating] excellence and innovation in teaching, learning, research and student development" [@trent_university_vision_2024]; with a determination "to put the learner at the centre of all [the university] does" [@university_of_guelph_mission_2024].

Unprincipled organizations do not need to question their choices of technology--as long as the technology advances whatever its goal is at any particular moment. But universities aim to achieve their mission while adhering to "integrity, quality, inclusiveness and teamwork" [@office_of_the_president_mcmaster_university_mission_2024]; with "vigilant protection for individual human rights, and a resolute commitment to the principles of equal opportunity, equity and justice" [@the_office_of_the_governing_council_secretariat_institutional_1992]; and  as they "foster sustainability, in its environmental, social and economic dimensions" [@university_of_guelph_mission_2024].

Universities must strike a careful balance between what they aim to achieve and how they achieve it. And so, what do AIs do that can advance the mission of universities without compromising their core values?

## What do GenAIs do?{#what-do-ais-do}

At this point, we might as well ask "what do AIs _not_ do"? 

A reason why many organizations have spent substantial resources considering the adoption of AIs is that they have been presented, quite deliberately, as _everything tools_: tools capable of doing many things a human can do, but faster and supposedly cheaper. While hammers are understood to have a limited range of uses that mostly have to do with blunt force, the release of GenAIs to the masses was accompanied by innumerable promises--quite a few of them made by entities and individuals that had a stake in the adoption of the technology^[Ranging from "AI is inevitable, and I will sell it to you" to "AI is inevitable, and for a consulting fee I will tell you how to level up with it".].

The public was promised that GenAI would "accelerate sustainability" [@nakagawa_accelerating_2023].

Also, that LLMs would transform sectors as diverse as health care, finance, human resources, insurance, "and beyond" [@ibm_what_2023]. AI-powered self-driving cars would reduce accidents, car ownership, pollution and noise, and would automate logistics and make parking tickets obsolete [@joshi_5_2022; @garsten_what_2024]. In mental health care, chatbots could complement human therapists by using patient data to analyze behavioral patterns to then mimic practitioner questions [@silva_4_2023]. AIs would revolutionize dating by helping "people connect and find potential partners" [@khalatian_matchmaking_2023].

The list of promises about what AI _can_ do has grown to encompass pretty much everything, from saving the planet from ourselves to micromanaging the [bowel movements of our pets](https://finance.yahoo.com/news/next-generation-ai-powered-cat-140000795.html).

```{r ai-litter-box, out.width="70%", fig.cap="\\label{fig:ai-bug-fix}Revolutionizing pet health"}
#| label: fig-revolutionizing-pet-care

knitr::include_graphics(glue::glue(here::here(), "/images/ai-litter-box.png"))
```

Colleges and universities have not been spared the onslaught of promises. 

@power_3_2024, presents three innovative uses for AI in higher education. First, AI could take over manual tasks and improve the budget by releasing administrative staff^[Probably from working at the university]. AI could evaluate applications for admissions, including assessing traits like the "grit" and "empathy" of applicants. AI could also personalize the experience of consumers by predicting their final grade with 60-70\% accuracy, and by providing corrective and instructive feedback. 

In another instance, @hie_how_2023 argue that AIs will likely be the primary way to access information in the future. For this reason, institutions must equip students with the skills like _prompt engineering_, the ability to craft effective queries that prompt a GenAI to provide the most useful outputs. Universities will use AIs to improve learning and challenge the critical thinking of students. Faculty will also benefit from AI: it will be used to evaluate their students, and will handle repetitive tasks like writing course syllabi.

@hie_how_2023 warn that "AI is here to stay"^[Writing in 2022, Mr. Hi√© predicted that the metaverse would be a [revolution in higher education](https://www.linkedin.com/pulse/metaverse-dedicated-pedagogy-revolution-higher-education-anthony-hi%C3%A9). This prediction aged poorly: on February, 2023, Microsoft [disbanded](https://www.theinformation.com/articles/microsoft-kills-its-industrial-metaverse-team-after-4-months) its Industrial Metaverse Core Team scantly four months after launching it. Shortly after that, Meta [slashed its own metaverse team](https://www.reuters.com/technology/meta-lay-off-employees-metaverse-silicon-unit-wednesday-2023-10-03/) in October, 2023.], and that "[faculty] might not be able to stop the AI trend from growing" so they might as well get on with the program and try to shape it as best they can.

Similarly hyperbolic claims about AI in higher education can be found peppered throughout the landscape [e.g., @adiguzel_revolutionizing_2023; @contact_north_five_2024; @dwivedi_opinion_2023; @johnson_ai_2023; ]. A fairly comprehensive list [from @johnson_ai_2023] is as follows (notice the use of "can" in the original):

- AI can provide personalized learning.  
- AI-powered systems can provide smart tutoring.  
- AI can automate grading.  
- AI can enable virtual classrooms.  
- AI can provide data insights for informed decisions.  
- Natural Language Processing (NLP) can enhance language learning.  
- AI can create interactive and adaptive learning materials.  
- AI can identify early signs of learning difficulties.  
- AI can aid in providing personalised professional development opportunities for teachers.
- AI can streamline administrative tasks.

Is it really possible that AIs can do all this?

## A note on the terms of the conversation {#terms-of-debate}

Discussions of AI as an _everything tool_ quickly devolve into a [Gish Gallop](https://en.wikipedia.org/wiki/Gish_gallop), a rhetorical device where a party makes an overwhelming number of arguments with little regard to their accuracy or strength. To be sure, AIs can do _some_ things well, but critics are forced to respond to every single propounded use with evidence, which usually is much more time consuming than making the unsubstantiated and/or misleading claims of the gallop itself. Proponents of AI can always admit that, ok, AIs do not actually do _that one thing_, but what about the myriad of other things that people say they really do? A second discursive device deployed by proponents of AI is an informal fallacy known as [The False Dilemma](https://en.wikipedia.org/wiki/False_dilemma): adopt AI or fail to level up.

In what follows, it is not my intention to confront every single possible use of AI in teaching and learning, since doing so plays into the intent of the Gish Gallop. As well, I reject the dilemma posed by the purveyors of hype, and I remain convinced that there are multiple ways to excel at teaching and learning that do not require AI, let alone the compromises demanded by AI.

## But what do GenAIs really do? {#really-what-do-they-do}

GenAIs are _deep learning models_ [@ibm_what_2021], so-called because they are neural networks with _depth_, meaning _many_ hidden layers. As an example, Large Language Models are models built with multiple layers of neural networks trained on _very_ large amounts of textual data [@ibm_what_2023].

Behind all the jargon, though, the neural networks that underpin GenAI are models designed to produce "statistically probable outputs" when prompted [@ibm_what_2021]. In more conventional terms, neural networks are regression models--much more sophisticated and powerful than linear regression to be sure--but regression models nonetheless [@ripley_neural_1994]. Here, it is important to note that the term regression refers to [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean)--regressing an input (e.g., a prompt) to a "statistically probable" outcome.

A regression model takes data as inputs, typically a "dependent" variable $y$ (observations of an outcome of interest) and "independent" variables $x$ (observations of things thought to correlate with the dependent variable). With these inputs, the model finds a conditional mean that becomes the output of the model, or $\hat{y}$ (i.e., a mean value conditional on the values of the dependent variables). The process of finding that conditional mean is to satisfy some criterion (e.g., that the conditional mean minimizes the error of the model--the distance between $y$ and $\hat{y}$).

@fig-bivariate-model is an example of the simplest regression model with only one independent variable (i.e., a bivariate model). The plot shows the pairs of $y$ and $x$ values, and the line is the model, i.e., the conditional mean $\hat{y}$. This model needs only two parameters: an intercept (the conditianl mean $\hat{y}$ when $x = 0$), and the slope of the line, in other words, the rate of change of $\hat{y}$ with respect to $x$. Each additional independent variable uses one additional parameter to describe the slope of the model with respect to that variable.

```{r simulate-simple-example}
set.seed(13253)

b0 <- 1.0
b1 <- 1.5

df <- data.frame(x = b0 + b1 * runif(100, 
                                0, 
                                10),
                 e = rnorm(100),
                 type = "training data") |>
  mutate(y = x + e)

x <- df |> pull(x)
y <- df |> pull(y)

mod <- lm(y ~ x, 
          data = df)
```

```{r plot-simulated-data, message=FALSE, out.width="70%", fig.cap="Bivariate Regression Model"}
#| label: fig-bivariate-model

ggplot(df,
       aes(x = x,
           y = y)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()
```

```{r data-vs-predictions-comparison, out.width="70%", fig.cap="Training data and model output compared"}
#| label: fig-data-vs-preds-01

df_pred <- data.frame(x = x)

y_hat <- predict(mod, df_pred)

df_pred_lr <- df_pred |> 
  mutate(y = y_hat,
         type = "regression output")

df_pred_all <- rbind(df |>
                   select(x, y, type),
                 df_pred_lr |>
                   select(x, y, type))

ggplot() + 
  geom_point(data = df_pred_all,
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Neural networks do something similar but using a more flexible net-like architecture. For instance, @fig-nn1 shows a relatively simple neural network trained using using the same data. This network is not "deep", as it has only one "hidden layer" comprised of two neurons between the input and the output layers. In comparison to the linear regression and its two parameters, this neural network uses seven parameters. Another key difference between a linear regression model and a neural network is that the latter requires an initial set of parameters, which are often drawn at random, thus making the results of the model contingent on the starting conditions of the training process.
```{r}
# Given the dataframe grid.1, let us now train a neural network. We need to first define a formula that specifies the dependent variable and the independent variables.
f <- as.formula("y ~ x")
```

```{r train-shallow-neural-network}
# Train a neural network using only two neurons in one hidden layer. The argument `linear.output` is set to TRUE, so that the activation function is not applied to the output node. The default activation function is the logistic function.
set.seed(33225433)

nn1 <- neuralnet(f, 
                data = df, 
                hidden = c(2), 
                linear.output = T) 
```

```{r save-shallow-neural-network}
# For some reason, the figure cannot be saved directly, so I saved it manually using screen capture
plot(nn1)
```

```{r plot-shallow-neural-network, out.width="70%", fig.cap="Simple Example of a Shallow Neural Network"}
#| label: fig-nn1

knitr::include_graphics(glue::glue(here::here(), "/images/nn1.png"))
```

```{r shallow-neural-network-predictions}
y_hat <- predict(nn1,
                 df_pred)

df_pred_nn1 <- df_pred |> 
  mutate(y = y_hat,
         type = "nn1 output")

df_pred_all <- rbind(df_pred_all |>
                   select(x, y, type),
                 df_pred_nn1 |>
                   select(x, y, type))
```

@fig-data-vs-preds-02 illustrates the predictions of the neural network in @fig-nn1, where we can see that the model is not limited to a linear response. This flexibility by the neural network is achieved by being less parsimonious (it uses more parameters).  

```{r data-vs-predictions-comparison-2, out.width="70%", fig.cap="Training data and model outputs compared: linear regression and shallow neural network"}
#| label: fig-data-vs-preds-02

ggplot() + 
  geom_point(data = df_pred_all,
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

More complex neural networks can be designed. @fig-nn2 shows a network deeper than the previous one, with two hidden layers, each consisting of five neurons. This model uses forty-six parameters. 
```{r train-deeper-neural-network}
# Train a neural network using only two neurons in one hidden layer. The argument `linear.output` is set to TRUE, so that the activation function is not applied to the output node. The default activation function is the logistic function.
set.seed(242)
nn2 <- neuralnet(f, 
                data = df, 
                hidden = c(5, 5), 
                linear.output = T)
```

```{r save-plot-deeper-neural-network}
# For some reason, the figure cannot be saved directly, so I saved it manually using screen capture
plot(nn2)
```

```{r plot-deeper-neural-network, out.width="70%", fig.cap="Example of a Deeper Neural Network"}
#| label: fig-nn2
 
knitr::include_graphics(glue::glue(here::here(), "/images/nn2.png"))
```

```{r}
y_hat <- predict(nn2,
                 df_pred)

df_pred_nn2 <- df_pred |> 
  mutate(y = y_hat,
         type = "nn2 output")

df_pred_all <- rbind(df_pred_all |>
                   select(x, y, type),
                 df_pred_nn2 |>
                   select(x, y, type))


```

The increased complexity leads to greater flexibility, and we can see that the model's predictions resemble more closely the actual data (@fig-data-vs-preds-03).  

```{r data-vs-predictions-comparison-3, out.width="70%", fig.cap="Training data and model outputs compared: shallow neural network and deeper neural network"}
#| label: fig-data-vs-preds-03

ggplot() + 
  geom_point(data = df_pred_all |>
               filter(type != "regression output"),
  aes(x = x,
      y = y,
      color = type,
      shape = type,
      size = type)) +
  scale_color_manual(values = c("training data" = "black",
                                "regression output" = "blue",
                                "nn1 output" = "chocolate1",
                                "nn2 output" = "brown")) +
  scale_shape_manual(values = c("training data" = 16,
                                "regression output" = 17,
                                "nn1 output" = 18,
                                "nn2 output" = 19)) +
  scale_size_manual(values = c("training data" = 1,
                                "regression output" = 2,
                                "nn1 output" = 2,
                                "nn2 output" = 2)) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r}
# How do these models perform?
mad_lr <- data.frame(df |>
             mutate(y_hat = df_pred_lr$y)) |>
  summarize(mse = sum(abs(y - y_hat)/n()))

mad_nn1 <- data.frame(df |>
             mutate(y_hat = df_pred_nn1$y)) |>
  summarize(mad = sum(abs(y - y_hat)/n()))

mad_nn2 <- data.frame(df |>
             mutate(y_hat = df_pred_nn2$y)) |>
  summarize(mad = sum(abs(y - y_hat)/n()))

delta_mad_nn1 <- (mad_nn1 - mad_lr)/mad_lr * 100

delta_mad_nn2 <- (mad_nn2 - mad_lr)/mad_lr * 100
```

Indeed, the first neural network is "better" than the linear regression in that it reduces the typical error by `r paste0(prettyNum(abs(delta_mad_nn1), digits = 2), "%")`, whereas the deeper network reduces the typical error by `r paste0(prettyNum(abs(delta_mad_nn2), digits = 2), "%")`, also with respect to the linear regression^[The "typical error" here is the mean absolute deviation of the predictions.].

There is a limit as to how flexible a model can be, and this is true of linear regression and of neural networks. Each additional parameter makes the model more flexible, but as a rule we cannot use more parameters than the number of data points that exist in the dataset used to train the model. Roughly, every additional parameter needs one additional datum to train on, and when there are as many parameters as data inputs the model becomes overfit and will predict each value of the independent variable in the training dataset perfectly. The model will fail to train if there are more parameters than data points.

In a neural network trained to work with natural language, the "predictors" are the words that come before and after a certain word. Several approaches exist to _embed_ a bag of words, which means using large text corpora to calculate the frequency of co-occurrence of words. These frequencies are then converted to numerical vectors that represent the position of a word in multidimensional space. These "coordinates" in the space of the vocabulary can then be used to calculate the "distances" between words. A recent breakthrough in the field of natural language processing is the development of _transformers_, a network architecture architecture that updates the numeric values of a word depending on the words that come before and after in a prompt.

Here are some key ideas to keep in mind when thinking about AIs:

- AIs are neural networks, which is to say, regression models.
- A regression model generates outputs that are regressions to the mean (i.e., statistically probable outputs).
- AIs are _very_ flexible non-linear regression models.
- Their flexibility comes from using "deep" architectures, which means many layers with many parameters^[According to reports [see @griffith_gpt_4_2023], GPT-1 had 117 million parameters, GPT-2 1.5 billion, and GPT-3 175 billion parameters. OpenAI no longer reports the number of parameters of its newer models, but in the latest version this is estimated to be in the trillions.]. The large number of parameters makes these models _extremely_ data hungry. 
- AIs are trained using an initial set of random values as parameters. The same training dataset can result in different models due to the randomness in the initial conditions.
- The flexibility of AIs means that outputs will vary seemingly at random with relatively small changes in the prompt. 
- Also, importantly, since regression models aim for the mean (i.e., a statistically probable output), they tend to be bad at _extrapolation_ (or thinking outside the box, if AIs could think). The further away an input (i.e., prompt) is from the space defined by the training dataset, the worse the output will tend to be^[Which is possibly why so-called hallucinations became more subtle as bigger datasets were used to train AIs. The increasingly large datasets essentially expanded the space where the model can interpolate.].

## GenAIs in teaching and learning {#ai-in-teaching-and-learning}

Some of the things that universities hope to achieve with AI include writing text (e.g. reference letters, grant applications, job descriptions, report summaries), conducting qualitative and quantitative data analysis, translation, captioning, providing multiple formats for learning and engagement, and for personalized support. Anecdotally, students report wanting to use generative AI not to complete assignments for them, but to help enhance learning in their courses by having concepts explained differently, by working on practice questions or by getting immediate feedback on drafted work. If we take a student-centered perspective, we might begin by asking about some of the things that AIs purportedly do for students.

## What do GenAIs do for students?{#what-do-ais-do-for-students}

- Students want concepts explained to them differently.

There are many ways for students to get different perspectives on concepts. They can form study groups, attend office hours to engage with their instructors, ask their Teaching Assistants, ask relatives and friends, and so on. But even for a student who for whatever reason cannot or will not do any of these things, the fact remains that relatively common concepts have already been explained by humans in dozens if not hundreds of different ways, often in writing that is available through their university library. Is an AI tool essential to find alternative explanations for common concepts? Only as a shortcut for a good library search (or to make up for the lack of an academic network, not taking advantage of office hours, etc.), and at the expense of research skills. As for more obscure concepts, given their paucity in training datasets, those would tend to be uncommon, and therefore improbable outputs (i.e. for reasons of lying far from the mean). This strongly suggests that any outputs about specialized queries should be treated with caution.

- Practice questions 

Most textbooks already include plenty of practice questions, especially for relatively common topics. For more specialized topics, asking interesting questions should be a skill in and of itself. An emphasis on practice questions, however, signals an underlying weakness of the process, namely, a focus on learning for the examination, as opposed to learning for its own sake. Posing interesting questions is an essential part of an instructor's job, but this part of the job becomes compromised when instructors are pushed to operate in massive classrooms. In this case, AI is a substitute, and demonstrably not a good one, for a qualified instructor. At issue, at well, is traditional evaluation approaches. A growing body of evidence points towards the effectiveness of alternative assessment schemes, such as continuous assessment through course work, and even seen examinations instead of the traditional unseen examinations [@turner_see_2018]. Alas, unseen examinations continue to dominate the evaluation landscape [@buckley_are_2023]. Ungrading, in particular, is increasingly recognized for its potential to help learners internalize the motivation to learn, and for fostering adaptability, creative thinking, and self-management [@gorichanaz_it_2024]. If studying for the examination often detracts from genuine learning, why are other assessment approaches seldom considered? Research by @vahed_continuous_2023 suggests that instructors need to develop assessment literacy, that is, an understanding of "the fundamental assessment concepts and procedures, and to engage in the ongoing process of adjusting instructional strategies to enrich students learning experiences". There is a indeed a dire need for innovative approaches in assessment, but it is unlikely that AI is the tool to develop them.

- Immediate feedback on drafted work

This is an utterly unrealistic expectation. Ideally, students should have the opportunity to discuss their drafted work with instructors before final submission (or could ask members of their study groups to proof-read their work, among many other ways to get feedback before submission). A conscientious instructor will invest time reading the drafted work to provide quality feedback. There is, however, a more insidious aspect about getting feedback from an AI...who, actually, provides the feedback in this case? Whose data are used to train the machine that provides the feedback? Who fine-tunes the machine that provides the feedback? Whose perspectives and views are used to provide the feedback, and can they ever be accountable for the feedback provided? A poignant example of this dilemma involves AI-based face recognition and online proctoring, as related by @hill_accused_2022.

- Creation of innovative learning materials

@mollick_instructors_2024 envision a future in which educators are innovators thanks to their adroit use of prompts for AIs. According to these authors, GenAI stands alone among educational technologies in that it can be "'programmed' through prompts alone", which should allow even instructors "without extensive technology or coding experience" to more easily create classroom applications. 

Leaving the technology fetish about classroom applications aside, Mollick and Mollick are well aware of the ethical concerns around using AI (see p. 6), including that "[m]odels are trained in ways that may violate copyright", that exploit the "efforts of low-wage workers in precarious conditions for part of their training", and the fact that "[m]odels are trained on biased data and can produce either subtly or overtly biased results". And yet, they choose to ignore every single one of these concerns for the sake of innovation. Machine-washed plagiarism that exploits low-wage workers while using inordinate amounts of energy is innovative only in a way that offends "integrity" [@office_of_the_president_mcmaster_university_mission_2024], "individual human rights...equity and justice" [@the_office_of_the_governing_council_secretariat_institutional_1992], and "sustainability, in its environmental, social and economic dimensions" [@university_of_guelph_mission_2024].

But what if the datasets and training processes used to develop GenAIs were unimpeachable, would the practice of generating learning materials using AIs then be "innovative"? Alas, no. As should be clear by now, any teaching materials generated in this way can only aspire to be average given the prompts used to get the machine going, and no one should buy the delusion that the use of this technology makes them innovators. In any case, the list of remedial measures required to verify that the materials generated are not nonsense is so long, that it is hard to see what is being gained [see @mollick_instructors_2024, pp. 4-5].

- Automated grading

There is nothing new about automated grading: [Scantron](https://www.scantron.com/company/) sheets for multiple choice examinations have existed for over four decades. They work well because the algorithm needed for grading multiple choices is simple, and importantly, non-random. There is no reason to use a neural network to grade multiple choice examinations. But on the flip side, there is also no evidence that a neural network can perform well at grading anything more complex than a multiple choice examination, since 1) it will tend to consider things that are closer to the mean as "better"; and 2) the amount of training data required for a classification task of this nature is massive. Labeling data is an essential, yet extremely expensive aspect of training GenAIs. The algorithm needs a "dependent" variable for training, a "label", which in this case a grade that a human assigned to a piece of work. Given all the possible variations of assignments (by year, by subject, by instructor), a general grading machine would require vast volumes of _labelled_ training data, and even so there is no guarantee that the results would be adequate, let alone appropriate.

- Writing reference letters

Using a chatbot to write reference letters must be one of the most pointless uses of chatbots, or one of the most dishonest. Imagine that the reference letter honestly included a disclosure that ChatGPT was used to write it. If the objective of a reference letter is for someone to attest from the vantage point of some level of expertise as to the skills and qualities of a person, a chatbot-written letter is probably worse than useless: what message does the disclaimer send to the recipient of the letter? And how much stock will anyone place on the veracity of the letter? To avoid this catch, one would have to occult or obscure the use of AI, an act that lays bare the dishonesty of the activity.

- Qualitative and quantitative data analysis

Computers have been doing mathematics reliably since at least the [Antikythera mechanism](https://en.wikipedia.org/wiki/Antikythera_mechanism) (ca. 178 BC),. Electronic computers have been doing mathematics for decades now, and neural networks have been used for decades to do data analysis. There is nothing new about that.  But the promise rather is that we will be able to do data analysis without having to _learn_ how to do data analysis. The idea that a chatbot could be used to do data analysis betrays a profound misunderstanding of both what chatbots do and what data analysis entails. Ask a chatbot to do some arithmetic: it will produce something that is right on average. But average is not always right. And seeing how AIs are now being trained on the dregs of the internet, it should be no surprise when an AI Overview responds to the prompt "2 + 2" with "potato".

```{r flickr-potato, out.width="70%", fig.cap="Data analysis using a chatbot? Maybe not such a good idea"}
knitr::include_graphics(glue::glue(here::here(), "/images/potato.png"))
```

An error like this is easy to spot. But we seldom use computers to do calculations that can be done by hand, and checking that an AI-generated response is not wrong is as time consuming as doing the analysis without an AI in the first place [see @yuan_how_2023].

From a different perspective, even the average output of an AI should be treated with caution. As noted [above](#really-what-do-they-do), the results of a neural network depend on the weights selected at random at the start of a training process. Change those weights, the results will change. As well, even small variations in the prompt can be impactful in models with billions or trillions of parameters. In statistical terms, any output is but one _realization_ of a random process. For the results to be useful, we should calculate the average of a large number of responses. Given the cost of training and querying a LLM, this is prohibitively expensive. 

But what about a task that involves logic instead of arithmetic? Research by @nezhurina_alice_2024 demonstrates that relatively simple natural language puzzles that are easily solved by humans, often result in confidently expressed errors when used to prompt an AI. Wrong answers, furthermore, are frequently backed up by confabulations, that is, nonsense expressed in reasoning-like terms. The onnections between words that LLMs deliver are but probabilistic word salads: @xu_hallucination_2024 use a formal approach to show that LLMs are inherently incapable of mapping outputs to a ground truth, and thus their results will only ever resemble a ground truth purely by chance. The outputs of a chatbot only sound plausible thanks to the vast amount of data points and parameters that the models use. But if the objective of qualitative research is to identify interesting aspects of the human experience, it is certainly not the case that they will be found in a regression to the mean.

- Identify early signs of learning difficulties.  

An AI should not be used to identify signs of learning difficulties for the same reasons that AIs cannot be trusted to identify text written by an AI. The false positive and negative rates are out of proportion compared to the benefits the practice might offer. A vigilant instructor will be aware of learning difficulties among their students. But were the process to become automated, who would decide when an intervention was warranted, and who qualified for it? Would the intervention be offered to everyone flagged by the system, including the false positives? And, could those denied an intervention complain? Importantly, who would fund an intervention, and who would deliver it? Another AI? 

- Personalized learning

Anyone receiving "personalized learning" from an AI can be sure that they will be treated as the machine-generated average of their class, and not as a person.

## What do GenAIs do to students? {#what-can-ais-do-to-students}

The preceding section illustrates how numerous tasks that AIs can notionally do in higher education are in fact poor replacements for a human-centered education. But what about the things that GenAIs can do _to_ students?

### Distract from learning and deskilling

Reliance on AI can short-circuit the learning process. Statements like "in the very near future AIs will likely be the primary way we access knowledge" [@hie_how_2023] convey the idea that prompt engineering will become an essential skill; perhaps even the skill to end all skills^[Prompt engineering is a pompous name for a task that more closely resembles reading the entrails of a Large Language Model than actual engineering. It is a task that cannot possibly be performed in a systematic way, and that offers no guarantees that it will work in the same way again, especially after a model is updated. Calling the activity of writing prompts "engineering" is a blatant attempt to steal the competence and reputation of actual engineering.].

It is important to note that, even if prompt "engineering" offered some value, the skill is tied to a singular technology and offers little transferability potential. @mollick_instructors_2024 demonstrate how prompt "engineering" is not even transferable _between_ different LLMs (see p. 5 and p. 36). Good skills transfer between contexts. Learning how to do an effective bibliographical query transfers between libraries. Learning to identify relevant content works while reading a novel, a book of history, or a text of econometrics. Writing, the slow and demanding process of converting thoughts into words, is essential to create a style of communication that is unique to each person. It might be argued, in fact, that writing _is_ learning [@klein_trends_2016].

In a sense, an AI is like a crutch. However, what the public is being promised is not a crutch: it is an All-Terrain Vehicle, but one that the user cannot hope to fully master due to its blackbox nature--in the sense that 1) neural networks are blackboxes; and 2) the development of AIs itself is a blackbox wherein things can more easily change based on the interests of their corporate owners than in response to the interests of their relatively small customers^[And this refers not only to individuals whose skills become yoked to ChatGPT and MidJourney. As an example, in 2022 McMaster University operated on revenues of less than [\$843 million](https://financial-affairs.mcmaster.ca/app/uploads/2023/06/2023-24-Consolidated-Budget-FINAL.pdf) (CAD). Microsoft, in contrast, operated on revenues of approximately [\$198,000 millions](https://www.microsoft.com/investor/reports/ar22/) (USD) in 2022. To put things in perspective (and using the average exchange rate of 2022), if McMaster used _all_ of its 2022 revenue to pay Microsoft, that would leave McMaster with zero dollars and still account for less than _one third of one percent point_ of Microsoft's revenue. A mid-sized Canadian university is like an ant scuttling under the economic boot of Microsoft.].

<!--
McMaster: 842,899,000 CAD
Microsoft: 198,000,000,000 USD == 257,776,200,000 CAD

https://www.exchange-rates.org/exchange-rate-history/usd-cad-2022
Avg. exchange rate in 2022: 1.3019 CAD/US

This is how much bigger Microsoft is compared to McMaster. 
(198000000000 * 1.3019)/842899000 

Put another way: If McMaster gave ALL OF ITS REVENUE to Microsoft, it would only account for 842899000 / (198000000000 * 1.3019) or less than one third of a percentage point of Microsoft's revenue.
--->

However, let us assume for a moment that using AIs in their current form does becomes an in-demand tool in the short term. What is the expected longevity of a skill like prompt "engineering"?

Besides a few methodological developments, improvement in the performance of AIs has mostly relied on using bigger datasets. This has led to a rush, as developers try to train their models with bigger datasets^[A rush that feels as frenetic as any gold rush in the past. However, while the gold rushes of the 19th century were relatively democratic due to their low entry costs, the current data rush has enormous entry costs and is largely dominated by large corporations and venture capital.]. But, despite the large number of text items and images available in digital form, there is still only a finite amount of data to be had.

As of this writing, the few big players that can afford to grab the data, are mostly done with all the data grabbing, sometimes by morally dubious, and at times by possibly illegal means [@clark_ex-amazon_2024]. And so, as the fountain of human-generated content has been reduced to a drip by a handful of corporations developing AIs, there has been a turn towards what might be the last untapped drops of information left on the internet.

OpenAI, a global leader in data grabbing, resorted to data from Stack Overflow [@stack_overflow_stack_2024], a website that [bills itself](https://stackoverflow.com) as "[a] community-based space to find and contribute answers to technical challenges". Said community was [unhappy](https://favtutor.com/articles/stack-overflow-community-not-happy-openai/) (to say the least) with the way Stack Overflow dealt with OpenAI, and has come to believe that the terms were abusive: while Stack Overflow's terms of service state that posters cannot revoke permission to use, the site also operates under a Creative Commons license that requires attribution, something that AIs notoriously _do not do_. It is an open question how long Stack Overflow will remain a community-based space after it started treating its members as unwitting, and even unwilling providers of data for OpenAI [@grimm_stack_2024].

Google, for its part, appealed to Reddit for training data to the tune of \$60 million per year [@roth_google_2024]. As Reddit content begins to creep into Google's AI-assisted search, the results of this deal have turned grotesque, with reports of AI Overview failures that include a suggestion to put non-toxic glue to pizza sauce to keep the cheese from sliding [@koebler__google_2024].

```{r google-reddit, out.width="70%", fig.cap="Google's AI Overview result was traced back to a post by redditor Fucksmith"}
knitr::include_graphics(glue::glue(here::here(), "/images/google-cheese-glue.png"))
```

Other AI Overview results included a recommendation by geologists to eat at least one small rock a day, a piece of advice tracked back to an article published by the well-known satirical periodical [The Onion](https://www.theonion.com/geologists-recommend-eating-at-least-one-small-rock-per-1846655112).

```{r google-the-onion, out.width="70%", fig.cap="Google's AI Overview result was traced back to an article in The Onion"}
knitr::include_graphics(glue::glue(here::here(), "/images/google-the-rock.jpg"))
```

Mind you, these, and numerous other hapless results, are produced by an AI that Google has already spent tens of billions of dollars developing [@vynck_after_2024]. To what extent can more money improve GenAIs? Probably not by much, since a key constraint is the amount of data available for model training [@villalobos_position_2024]. And dismal as this is, the outlook may be even worse than simply a frantic race after more human-generated data. As GenAIs become more widely used, they can produce data at a pace that humans cannot match, much of which is already finding its way into the internet in the form of text and images--and not only through clickbait websites and disinformation [@ruffo_studying_2023], but also in scientific writing, supposedly the most thoroughly vetted form of writing in the world [@cabanac_tortured_2021; @maiberg_ai_2024].

What can we expect as future GenAI models begin to be trained using data that was generated by previous GenAIs? Revisiting the examples in a preceding section ([But really, what do AIs do?](#really-what-do-they-do)) we can see that the predictions of a model are more uniform than the original data used for training. Technically, the variance of a model's output is substantially reduced compared to the variance of the training data: the outputs of a model will be less varied, less diverse, and more norm-conforming. The implication should be clear: training models with the output of models is a recipe for increasingly bland outputs. We may be witnessing _Peak ChatGPT_, and future model versions, instead of improving, may begin to regress to duller versions of their own previous outputs. To compound things, as backlash against a data rush that is perceived as exploitative and abusive grows, new tools are being developed to generate data that are manipulated in ways that are too subtle to be detectable by a human, but that are deleterious to the training process of a GenAI [@salman_raising_2023], including _PhotoGuard_ [@heikkila_this_2023], _Nightshade_ [@shan_nightshade_2024], _Shadowcast_ [@xu_shadowcast_2024].

All in all, the evidence suggests that the scarcity of fresh data, paired with more abundant model-generated and/or poisoned data, will make the performance of these machines stagnate or deteriorate. Which begs the following questions. How useful will AIs be in the future? How persistent will skills like prompt "engineering" be? Will learning how to prompt an AI be worth sacrificing the opportunity to develop transferable skills like coding, writing, and thinking? Who, after all, stands to benefit the most by the form of deskilling that AIs encourage?

## What do GenAIs do for the mega-corporations pushing them? {#what-do-ais-to-corporations}

AIs do not seem to do much of value for students, as they can be wasteful at best and harmful at worst. But the technology must do something for someone, otherwise it would not be ubiquitous. So what do AIs do for the the organizations that develop them? So far, the breakneck race to develop AI has soaked immense amounts of venture capital and other speculative money, in addition to incalculable resources by many other entities^[Some observers have even wondered whether the AI fever has not actually become detrimental to innovation [@siegel_ai_2023; @vinsel_dont_2023].]. Between 2012 and 2020, venture capital investments in AI grew twenty five-fold, from \$3 billion (USD) to \$75 billion (USD)^[For perspective, Ontario's Ministry of Colleges and Universities, whose mission includes supporting "research and innovation to help the province compete and thrive in the global economy", spent $163,482,281 (CAD) on its research program in 2021-2022 [@ontario_ministry_of_colleges_and_universities_expenditure_2023], a mere blip (~0.0017\%) that barely registers compared to venture capital investments in AI in 2020 alone.] [@tricot_venture_2021]. 

Alas, the industry has not necessarily made money for investors yet. 

@berber_peter_2024, for instance, reports that the AI industry spent \$50 billion (USD) in 2023 on chips alone, while bringing in only $3 billion in revenue. @patel_inference_2023 estimated that incorporating ChatGPT-like LLMs into search would lead to a loss of \$30 billion (USD) in profit for Google. At a guess, the industry is running at a loss in almost every respect: in the same report, @patel_inference_2023 calculated the cost of running a query in ChatGPT at about \$0.36--so it would take just about 57 monthly queries per user with a "Plus" subscription ([\$20/month](https://openai.com/chatgpt/pricing/)) for OpenAI to begin losing money on that one customer. And a large number of users are not even paying subscribers: based on revenue reports [see @perez_chatgpts_2024] there were at most 210,000 subscribers to the "Plus" plan in 2024, out of a user base estimated at tens of millions [@tong_exclusive_2023]. The profitability of AI would look even worse if not for the fact that the cost of labor is suppressed by outsourcing the most intensive, disturbing, degrading, and damaging aspects of training AIs to the Global South [@nix_analysis_2024].

Venture capital is not known for its charitable aims, though, and at some point investors will want their money back with a heap of profit on top [@sriram_ai_2023]. Who will pay to make investors whole? And how much those who end up paying will need to pay? Or, in the worst case, will _have_ to pay, if their skills become tied to this one technology? Deskilling, both of individuals and of organizations, appears to be one of the few pathways to AI profitability: once people have failed to learn a skill, and therefore become dependent on the tool, it becomes possible for the maker of the tool to turn into a digital rentier [@sadowski_internet_2020; @komljenovic_rise_2021]. From this perspective, what is on offer by the AI industry is SaaS--Skill as a Service.

## What do GenAIs do to everyone and everything else? {#what-do-ais-do-to-everyone-else}

### AI is an eminently extractive and exploitative industry

ChatGPT 4.o was announced on May 13, 2024, with great fanfare and a flashy display of the model's new capabilities, which included functions to respond vocally to prompts, and to identify images via a mobile device's camera. 4.o's voice, in particular, caused considerable titillation [@roose_is_2024]. The voice was modeled after the digital personal assistant of the movie _Her_, Sam Altman's favorite movie by his own account. The background story of 4.o's voice is more sordid: Altman reportedly tried to hire Scarlett Johannson, who played the AI assistant in the movie, to be the voice of ChatGPT 4.o [@kastrenakes_scarlett_2024], but when she declined, OpenAI went and used a mimicry of [her voice](@as-an-aside) anyways. This voice was retired after Ms. Johansson threatened legal action [@allyn_scarlett_2024]. 

Alas, very few have the wealth and social standing of Scarlett Johansson to challenge OpenAI. Most people in the world lack that power and the AI industry has not been shy to exploit the imbalance [@roberts_commercial_2016; @germain_google_2023; @weatherbed_adobes_2024]. 

Besides being extractive, the AI industry is exploitative in darker ways. ChatGPT was trained on vast amounts of data scooped from the internet, where many societal problems are amplified. Trained on horrible content, it was hard to sell a ChatGPT prone to racist, sexist, obscene outbursts. Fortunately for OpenAI, companies like facebook had already hit on a solution of sorts to reduce the cost of moderation. The idea was basically to train AIs to detect content that was unpalatable. Indeed, they had found that feeding an AI with labeled examples of all the worst that exists on the internet (e.g., violence, hate speech, child sexual abuse) could produce tools that learned to detect various manifestations of such toxicity in the wild, to filter it out before it reached users. However, to obtain the essential labeled inputs, OpenAI sent tens of thousands of samples to Kenya, with text that described murder, suicide, bestiality, self-harm, incest, torture, and child sexual abuse situations in graphic detail.

What did this do for and to the people doing all the labeling in one of the poorest regions in the world? First of all, it provided an income: "data labelers employed...on behalf of OpenAI were paid a take-home wage of between around $1.32 and $2 per hour depending on seniority and performance." But at what cost? As one worker tasked with reading and labeling text for OpenAI told TIME, "he suffered from recurring visions after reading a graphic description of a man having sex with a dog in the presence of a young child. 'That was torture,' he said. 'You will read a number of statements like that all through the week. By the time it gets to Friday, you are disturbed from thinking through that picture'" [@perrigo_2_2023].

### Privacy issues

LLM are known to regurgitate verbatim chunks of the training datasets. This is called _memorization_, or in more conventional terms, model overfitting--an extreme regression to the mean that excels at predicting data in the training set. Overfitting happens when a model has too many parameters or too few data inputs. Data "memorization"--the verbatim or quasi-verbatim output of training data--can be as high as 7% in the case of some LLMs [@peng_near_duplicate_2023, p. 8]. This means that data cannot be presumed to be private once that they have been used to train an AI.  And this issue is just a possible result of using overfitted models. But what about data leaks? So far, organizations such as the National Archives and Records Administration in the US, the US Congress, and companies like Samsung and Apple have restricted or banned the use of ChatGPT and Copilot for data security reasons [@koebler_national_2024; @singh_after_2024]: no data transferred in the form of a prompt can be presumed to be safe.

### Environmental impacts

Every human activity has an environmental impact, and at a time when we face unprecedented challenges with climate change, we must be discerning in how we consume resources. The choice is clear and Microsoft has told us: we need to burn the planet [@smith_our_2024] to create the AIs that will help us save the planet [@nakagawa_accelerating_2023].

Researchers like @crawford_generative_2024 have already sounded the alarm: the environmental costs of AI are soaring, and doing so mostly in secret [@luccioni_power_2024]. It has been estimated that ChatGPT consumes up to 25 times more energy than a regular web search,, and according to some estimates, the whole of AI may consume twice as much energy as the whole of France by 2030 [@brussels_chatgpt_2024]. As the examples in the preceding sections illustrate, this is in exchange for results that are not better than a simple search, and often are considerably worse.

### Reproduction of unjust structures of power

When using a Generative AI we must ask whose perspectives the tool will tend to reproduce and amplify. In this sense, numerous observers [e.g., @ahuja_we_2023; @stross_tech_2023; @gebru_tescreal_2024] have noted the ideological leanings of various actors in the AI scene, an elite world of tech investors^[A group that author Charles Stross colorfully [characterizes](https://www.antipope.org/charlie/blog-static/2024/04/the-radiant-future-of-1995.html) as "fascist-adjacent straight white males with an unadmitted political agenda". This characterization is somewhat unfair: both [Sam Altman](https://www.advocate.com/news/sam-altman-openai-exclusive) and [Peter Thiel](https://www.npr.org/2016/07/21/486966882/i-am-proud-to-be-gay-tech-investor-peter-thiel-tells-gop-convention) are openly gay. On the other hand, their political agenda hides in plain sight. @thiel_education_2009 has written about how he "no longer believe[s] that freedom and democracy are compatible". Sam Bankman-Fried's FTX donated millions to a group with racist ties [@wilson_sam_2024]. Elon Musk once shared a meme comparing Canada PM Justin Trudeau to Hitler while defending the vandals who terrorized Ottawa in 2022 [@henderson_and_elon_2022].] known for fringe ideologies that combine uber-techno-optimism and existential fright (including transhumanism, extropianism, singularitarianism, and cosmism)^[A quick summary of these beliefs is that those who can afford it, are destined to use technology to indefinitely extend life and conquer the stars...but maybe not before super-intelligent AIs destroy humanity.] with extreme theories of justice (including rationalism, effective altruism^[A movement exposed to the harsh light of public opinion for being the philosophy espoused by convicted fraudster and money launderer Sam Bankman-Fried [@crook_effective_2023].], and longtermism^[A drastic form of utilitarianism convinced that the well-being of trillions of humans in an imagined distant, star-faring future, is well worth the sacrifices of a few billions of humans today.]). 

One does not need to posit a conspiracy theory to worry about the influence (coordinated or not) of a small group of uber-wealthy people who collectively can muster more than _half a trillion dollars_ as they try to turn their beliefs into a reality for all [@stross_tech_2023]. So leaving aside the question of a potential "unadmitted political agenda", there is still the immediate problem that the datasets used to train AIs, much like the technology itself, are not politically neutral. _Platformed racism_ [@matamoros-fernandez_platformed_2017], for instance, is known to reproduce malignant biases. Platformed racism results when the platforms are exploited to create and spread racist ideology, often in a grass-roots fashion, but more ominously with the assist of bad-faith actors^[Online Russian disinformation has already had a wide reach [@treyger_russian_2022] and furthermore is known to operate by seeding distrust between ethnic communities [@svedkauskas_analysis_2020]. A U.S. Department of State report notes that Russian intelligence is known to exploit "all kinds of separatism and ethnic, social and racial conflicts, actively supporting all dissident movements--extremist, racist, and sectarian groups" to destabilize internal U.S. politics [@us_department_of_state_gec_2024, p. 51].]. Given how cagey corporations are about the data they use to train their models, it is anyone's guess how much dis- and misinformation has already been passed to their AIs^[This is part of the Unfathomable Data problem discussed by @bender_dangers_2021.]. The second aspect of platformed racism is the implicit endorsement of racist ideology through vague standards and arbitrary moderation practices^[According to the Center for Countering Digital Hate, platforms like facebook, Instagram, Tiktok, Twitter, and YouTube, failed to act 84% of the time on hundreds of posts that had millions of views, with facebook being the worst performer [@center_for_countering_digital_hate_failure_2021].] that exclude some views while allowing others to thrive [@myers_west_censored_2018; @hawkins_race_2023].

Search engines already tended to reinforce racism [@noble_algorithms_2018]; given the above, no one should expect AI-augmented search engines to do any better^[A report by the Center for Countering Digital Hate found that Google's Bard generated wrong and/or hateful outputs in 78 out of 100 tests [@center_for_countering_digital_hate_googles_2023]. In addition, AI is also a tool ripe for abuse. Google's own researchers have come to the inevitable conclusion that it is easier to produce disinformation with AI, than it is to use AI to combat it [@maiberg__google_2024].]. In another wretched incident, Google's AI Overview stated that Barack Obama was the first Muslim president of the US, a regrettable piece of misinformation that follows on a years-long Russian-assisted right-wing propaganda campaign to otherize Mr. Obama [@tenbarge_glue_2024]. 

Mishaps like this, and others, are dismissed by Google as attempts "to trip up the technology with uncommon questions" [@tenbarge_glue_2024]. This dismissal only reinforces the notion that AIs are only ever useful when [regressing to the mean](#really-what-do-they-do). Responses like fiddling with the model (fine tuning it) to provide more palatable outputs^[Like in the case of [Google's response](https://blog.google/products/search/ai-overviews-update-may-2024/) to the poor quality of its recently launched AI overviews.] are not reassuring. Who is to say what a handful of unaccountable corporations will find unpalatable tomorrow given their insatiable quest for profits, let alone in the face of demands by powerful authoritarian regimes?

## Concluding remarks

Neural networks and machine learning are nothing new, and they have their uses. But those uses have been suffocated, and to some extent discredited, by the current hype around Generative AI. This hype has been carefully manufactured [@morrone_signals_2024], and has been amplified by media tools willing to believe in magic mirrors [@burneko_if_2024]. There are now calls to press pause on the hype [@angwin_press_2024] but these voices are barely audible above the thundering echos inside the AI hype chamber.

Universities and colleges would be wise to heed the advice to press the pause button on the AI hype to avoid falling prey to a confidence trick. Higher education is often driven by a deeply ingrained belief in innovation (even, or perhaps particularly, for its own sake), and this impulse is compounded by the way AI adoption has been drawn as a false dilemma, thus triggering a fear of missing out. There are other pressures at work, including a decades-long history of manufactured crises in education, in Canada as elsewhere [@cizek_give_1999; @usher_state_2023].

In this essay I have tried to describe the numerous issues with AI as an _everything tool_.

AI, as marketed by the likes of OpenAI, Googla, and Microsoft, cannot be used in a way that is consistent with values like integrity, respect for human rights, and environmental sustainability. This is well-known: most discussions of AI in education acknowledge that their use is rife with problems. For instance, @mollick_instructors_2024 note issues with copyright, exploitation of low-wage workers, reproduction of biases, the risk of automation bias, and privacy loss. These are issues that "educators may want to consider before deciding whether to use these systems". This disclaimer highlights the important role of institutions to provide the moral and ethical clarity that may elude individual educators working under desperately under-resourced conditions, or simply willing to win a race to the bottom. And yet, I have little illusion that these arguments will prove persuasive. 

I would still appeal, though, to the university's self-regard and sense of self-preservation. 
We would do our students an unforgivable disservice by offering them a skill that reversed them to mediocrity and left them at the mercy of predatory corporations. "This university uses ChatGPT to teach" is simply a way of saying that we are too cheap to offer a human-centered education. But in truth, a university is too expensive an institution to be attractive under such conditions, when there are far less expensive alternatives for an AI-based education ("for only $9.99 proof yourself against layoffs").

Going down this path would be a regrettable mistake at a time when universities are under siege from unsympathetic or actively hostile actors. A university‚Äôs main source of authority is its reputation as a place that preserves and expands knowledge in a principled way. If we give this principle away, what would be left to sustain universities as institutions that add value to society?

## References
